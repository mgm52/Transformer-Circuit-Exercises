\documentclass[11pt]{article}
\usepackage{a4wide,parskip,appendix}
\usepackage{amsfonts}
\usepackage{pdfpages}
\usepackage{mdframed}
\usepackage{titlesec}
\usepackage{venndiagram}
\usepackage{float}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{caption}
\usepackage{algorithm2e}
\usepackage{xcolor}
\usepackage[a4paper, total={5in, 9in}, left=0.75in, right=0.75in]{geometry}
\usepackage{csquotes}
\MakeOuterQuote{"}

\titlespacing*{\section}
{0pt}{5.5ex plus 1ex minus .2ex}{4.3ex plus .2ex}
\titlespacing*{\subsection}
{0pt}{5.5ex plus 1ex minus .2ex}{2.3ex plus .2ex}

\newenvironment{aside}
  {\begin{mdframed}[style=0,%
      leftline=false,rightline=false,leftmargin=2em,rightmargin=2em,%
          innerleftmargin=0pt,innerrightmargin=0pt,linewidth=0.75pt,%
      skipabove=12pt,skipbelow=12pt,fontcolor=darkgray]\small}
  {\end{mdframed}}
\newenvironment{answerbox}%
  {\begin{mdframed}[linecolor=darkgray,%
                    roundcorner=10pt,innertopmargin=10pt,%
                    innerbottommargin=10pt,skipabove=12pt,skipbelow=12pt]}%
  {\end{mdframed}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\renewcommand{\vec}[1] {\mathbf{#1}}
\newcommand{\asidetitle}[1]{\large{\textit{#1}}\small}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\textfn}[1]{\textsc{\small{#1}}}
\newcommand{\bigmath}[1]{\colorbox{lightgray}{$\mathbf{#1}$}}
\newcommand{\highlight}[2][yellow]{\mathchoice%
  {\colorbox{#1}{$\displaystyle#2$}}%
  {\colorbox{#1}{$\textstyle#2$}}%
  {\colorbox{#1}{$\scriptstyle#2$}}%
  {\colorbox{#1}{$\scriptscriptstyle#2$}}}%

\RestyleAlgo{ruled}

\begin{document}

\title{\textbf{Transformer Circuit Exercises}}
\author{A Latex writeup of \href{https://transformer-circuits.pub/2021/exercises}{transformer-circuits.pub/2021/exercises}}
\date{August 16, 2024}
\maketitle

% \begin{abstract}
% This is a Maths problem.
% \end{abstract}

\section*{Exercises}

\subsection*{Warm Up}

\begin{itemize}
    \item Describe the transformer architecture at a high level.

    \item Describe how an individual attention head works in detail, in terms of the matrices $W_Q$, $W_K$, $W_V$, and $W_{out}$. (The equations and code for an attention head are often written for all attention heads in a layer concatenated together at once. This implementation is more computationally efficient, but harder to reason about, so we'd like to describe a single attention head.)
    
    \item Attention heads move information from a subspace of the residual stream of one token to a different subspace in the residual stream of another. Which matrix controls the subspace that gets read, and which matrix controls the subspace written to? What does their product mean?
    
    \item Which tokens an attention head attends to is controlled by only two of the four matrices that define an attention head. Which two matrices are these?
    
    \item Attention heads can be written in terms of two matrices instead of four, $W_Q^T \cdot W_K$ and $W_{out} \cdot W_V$. In the previous two questions, you gave interpretations to these matrices. Now write out an attention head with only reference to them.
    \begin{itemize}
        \item What is the rank of these matrices?
    \end{itemize}
    
    \item You'd like to understand whether an attention head is reading in the output of a previous attention head. What does $W_V^2 \cdot W_{out}^1$ tell you about this? What do the singular values tell you?
\end{itemize}



\subsection*{Exercise 1 - Building a Simple Virtual Attention Head}

Small transformers often have multiple attention heads which look at the previous token, but no attention heads which look at the token two previous. In this exercise, we'll see how two previous token heads can implement a small "virtual attention head" looking two tokens behind, without sacrificing a full attention head to the purpose.

Let's consider two attention heads, head 1 and head 2, which both attend to the previous token. Head 1 is in the first layer, head 2 is in the second layer. To make it easy to write out explicit matrices, we'll have the k, q, and v vectors of both heads be 4 dimensions and the residual stream be 16 dimensions.

\begin{itemize}
    \item[(a)] Write down $W_V^1$ and $W_{out}^1$ for head 1, such that the head copies dimensions 0-3 of its input to 8-11 in its output.

    \item[(b)] Write down $W_V^2$ and $W_{out}^2$ for head 2, such that it copies 3 more dimensions of the previous token, and one dimension from two tokens ago (using a dimension written to by the previous head).

    \item[(c)] Expand out $W_{net}^1 = W_{out}^1 \cdot W_V^1$ and $W_{net}^2 = W_{out}^2 \cdot W_V^2$. What do these matrices tell you?

    \item[(d)] Expand out the following matrices: Two token copy: $W_{net}^2 \cdot W_{net}^1$. One token copy: $W_{net}^2 \cdot \text{Id} + \text{Id} \cdot W_{net}^1$.

    \item Observation: When we think of an attention head normally, they need to dedicate all their capacity to one task. In this case, the two heads dedicated 7/8ths of their capacity to one task and 1/8th to another.
\end{itemize}



\subsection*{Exercise 2 - Copying Text with an Induction Head (Pointer Arithmetic Version)}

The simplest kind of in-context meta-learning that neural networks do is increasing the probability of sequences they've seen before in this context. This is done with an "induction head" that looks at what followed after last time we saw a token.


There are at least two algorithms for implementing induction heads. In this exercise, you'll build up the "pointer arithmetic" algorithm by hand.

\begin{itemize}
    \item[(a)] Let $u_0^{cont}, u_1^{cont}, \ldots, u_n^{cont}$ be the principal components of the content embedding. Write $W_Q$ and $W_K$ for an attention head (with 4 dimensional queries and keys) selecting tokens with similar content to the present token, including the present token itself.
    
    \item[(b)] Let $u_0^{\cos}, u_0^{\sin}, u_1^{\cos}, u_1^{\sin}, \ldots$ be a basis describing the position embedding in terms of vectors that code for the sine and cosine embedding of the token position (e.g., $\lambda(\cos(\alpha_0 n_{tok}))$ with descending magnitudes. Write $W_Q$ and $W_K$ for an attention head (with 4 dimensional queries and keys) that self-selects the present token position.
    
    \item[(c)] Using the position embedding basis described in (b), write $W_Q$ and $W_K$ for an attention head (with 4 dimensional queries and keys) that self-selects the \emph{previous} token position. Hint: think about a 2D rotation matrix.
    
    \item[(d)] Write $W_Q$ and $W_K$ for an attention head (with 8 dimensional queries and keys) selecting tokens with similar content to the present token, but disprefers attending to itself. Hint: refer to (b) and use extra 4 dimensions for keys and queries.
    
    \item[(e)] Write down $W_V$ and $W_{out}$ for the attention head you described in (d), such that it extracts the largest 8 dimensions of the position embedding from the token it attends to, and writes them to the vectors $v_0, v_1, \ldots$.
    
    \item[(f)] Write $W_Q$ and $W_K$ for an attention head which attends to the token after a previous copy of the present token. Hint: use the outputs of the head from (e) and the strategy you used in (c).
\end{itemize}



\subsection*{Exercise 3 - Copying Text with an Induction Head (Previous Token K-Composition Version)}

Some positional encoding mechanisms, such as rotary attention, don't expose positional information to the $W_V$ matmul. Transformers trained with these mechanisms can't use the strategy from (e) and (f) in the previous exercise to manipulate positional encoding vectors.

For these transformers, we've seen an alternate mechanism, where the first head copies information about the preceding token into a subspace, and the second head uses that subspace to construct queries and keys. Assuming the same positional encoding mechanism as above, write down $W_Q^{1},W_K^{1},W_V^{1},W_O^{1}$ and $W_Q^{2}$ and $W_K^{2}$ for a pair of attention heads implementing this algorithm.

\end{document}